---
title: "Reinforcement Learning Task 1: From Neural Networks to Q-Values"
subtitle: "Exercises 1-7: Foundation Concepts, Market Data, Neural Networks, and Q-Learning Basics"
author: "Student Name"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
# Setup for Knitr
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  cache = TRUE
)

# Set options
options(
  knitr.duplicate.label = "allow"
)
```

## Overview

This R Markdown document addresses the first seven exercises from Task Sheet 1 of the Reinforcement Learning seminar. It focuses on building foundational understanding of Markov chains, preparing market data, creating state vectors, setting up a neural network for Q-value approximation, generating artificial Q-value targets, outlining a simplified Q-learning training loop, and implementing the epsilon-greedy action selection strategy.

Following Domain-Driven Design (DDD) principles, code is organized around core domain concepts: **Agent**, **Environment**, **State**, and **Action**.

## Library Setup

```{r libraries}
# Load required libraries
# Ensure these libraries are installed. If not, uncomment and run install.packages()
# install.packages(c("quantmod", "dplyr", "ggplot2", "lubridate", "tibble", "moments", "neuralnet"))

library(quantmod)    # For financial data acquisition
library(dplyr)       # For data manipulation
library(ggplot2)     # For visualization
library(lubridate)   # For date handling
library(tibble)      # For modern data frames
library(moments)     # For skewness and kurtosis
library(neuralnet)   # For neural network implementation

# Set a consistent random seed for reproducibility
set.seed(42)
```

# Exercise 1: Markov Chain Fundamentals

## What is a Markov Chain?

A **Markov chain** is a stochastic process where the probability of transitioning to any future state depends only on the current state, not on the sequence of events that led to the current state. This property is known as the **Markov property** or "memorylessness."

Mathematically, for a sequence of random variables X₀, X₁, X₂, ..., the Markov property states:
**P(Xₙ₊₁ = x | X₀, X₁, ..., Xₙ) = P(Xₙ₊₁ = x | Xₙ)**

## Financial Example: Credit Rating Transitions

A classic example in finance is **credit rating transitions**. Consider a bond's credit rating (AAA, AA, A, BBB, BB, B, CCC, Default):

- The probability of a bond moving from rating A to rating BBB next year depends only on its current rating (A)
- It does not depend on whether the bond was previously rated AAA, AA, or has always been rated A
- This makes credit rating systems a natural application of Markov chains

```{r credit-transitions}
# Domain Service: Credit Rating Model
# Responsibility: Create and manage credit rating transition probabilities
create_credit_transition_matrix <- function() {
  # Transition probabilities (simplified for demonstration)
  transition_matrix <- matrix(c(
    0.90, 0.08, 0.01, 0.01, 0.00, 0.00,  # From AAA
    0.01, 0.87, 0.10, 0.01, 0.01, 0.00,  # From AA  
    0.00, 0.02, 0.85, 0.11, 0.01, 0.01,  # From A
    0.00, 0.00, 0.03, 0.80, 0.15, 0.02,  # From BBB
    0.00, 0.00, 0.00, 0.05, 0.75, 0.20,  # From BB
    0.00, 0.00, 0.00, 0.00, 0.00, 1.00   # From Default (absorbing)
  ), nrow = 6, byrow = TRUE)
  
  rownames(transition_matrix) <- c("AAA", "AA", "A", "BBB", "BB", "Default")
  colnames(transition_matrix) <- c("AAA", "AA", "A", "BBB", "BB", "Default")
  
  return(transition_matrix)
}

# Display the transition matrix
credit_transitions <- create_credit_transition_matrix()
print("Credit Rating Transition Matrix:")
print(round(credit_transitions, 3))

# Verify rows sum to 1 (property of stochastic matrices)
cat("\nRow sums (should all equal 1.0):\n")
print(rowSums(credit_transitions))
```

This example demonstrates the Markov property: a bond rated 'A' has the same probability distribution for next year's rating regardless of its rating history.

# Exercise 2: Market Data Preparation

## Data Acquisition and Return Calculation

```{r market-data-acquisition}
# Domain Service: Market Data Provider
# Responsibility: Acquire and clean market data
acquire_spy_data <- function(start_date = "2015-01-01") {
  # Input validation
  if (!is.character(start_date)) {
    stop("start_date must be a character string in YYYY-MM-DD format")
  }
  
  tryCatch({
    cat("Downloading SPY data from", start_date, "...\n")
    
    # Download SPY data using quantmod
    spy_data_xts <- getSymbols("SPY", 
                               src = "yahoo", 
                               from = start_date, 
                               auto.assign = FALSE, 
                               warnings = FALSE)
    
    # Extract adjusted closing prices (accounts for dividends and splits)
    spy_prices <- Ad(spy_data_xts)
    
    # Convert to data frame with proper date handling
    spy_df <- data.frame(
      date = index(spy_prices),
      price = as.numeric(spy_prices)
    ) %>%
      as_tibble() %>%
      arrange(date) %>%
      filter(!is.na(price))  # Remove any NA values
    
    cat("Successfully downloaded", nrow(spy_df), "price observations\n")
    cat("Date range:", min(spy_df$date), "to", max(spy_df$date), "\n")
    
    return(spy_df)
    
  }, error = function(e) {
    stop("Failed to download SPY data: ", e$message)
  })
}

# Acquire the market data
spy_data <- acquire_spy_data("2015-01-01")

# Display basic information about the dataset
cat("SPY Dataset Summary:\n")
cat("Number of observations:", nrow(spy_data), "\n")
cat("Date range:", as.character(min(spy_data$date)), "to", as.character(max(spy_data$date)), "\n")
cat("Price range: $", round(min(spy_data$price), 2), "to $", round(max(spy_data$price), 2), "\n")

# Show first and last few observations
head(spy_data)
tail(spy_data)
```

```{r return-calculation}
# Domain Service: Return Calculator
# Responsibility: Convert prices to percentage returns
calculate_percentage_returns <- function(price_data) {
  # Input validation
  if (!is.data.frame(price_data) || !"price" %in% names(price_data)) {
    stop("Input must be a data frame with a 'price' column")
  }
  
  if (nrow(price_data) < 2) {
    stop("Need at least 2 price observations to calculate returns")
  }
  
  # Calculate daily percentage returns using the formula: (P_t / P_{t-1}) - 1
  price_data %>%
    arrange(date) %>%
    mutate(
      # Lag price for return calculation
      price_lag = lag(price, 1),
      
      # Calculate percentage return: (P_t / P_{t-1}) - 1
      return_pct = (price / price_lag - 1) * 100,
      
      # Calculate log return as alternative: ln(P_t / P_{t-1})
      return_log = log(price / price_lag) * 100
    ) %>%
    # Remove first observation (no return can be calculated)
    filter(!is.na(return_pct)) %>%
    select(date, price, return_pct, return_log)
}

# Calculate returns for SPY data
spy_returns <- calculate_percentage_returns(spy_data)

cat("Return Calculation Summary:\n")
cat("Number of return observations:", nrow(spy_returns), "\n")
cat("Mean daily return:", round(mean(spy_returns$return_pct), 4), "%\n")
cat("Standard deviation:", round(sd(spy_returns$return_pct), 4), "%\n")
cat("Min return:", round(min(spy_returns$return_pct), 2), "%\n")
cat("Max return:", round(max(spy_returns$return_pct), 2), "%\n")

# Display first few return observations
head(spy_returns, 10)
```

## Why Returns Are Better Input Than Raw Prices

Returns are superior to raw prices for several fundamental reasons:

1. **Stationarity**: Returns are more likely to be stationary (constant statistical properties over time), while prices typically exhibit trends and non-stationarity
2. **Scale Independence**: Returns are dimensionless percentages, making them comparable across different assets and time periods
3. **Financial Relevance**: According to the glossary, **Reward** represents "the immediate payoff after an action." In trading, this payoff is the return on investment, not the absolute price level
4. **Markov Property**: Returns better satisfy the Markov assumption that the current state contains sufficient information for decision-making
5. **Risk Management**: Financial risk models are built around return distributions, not price levels

## Data Visualization and Analysis

```{r return-analysis-plots}
# Domain Service: Visualization
# Responsibility: Create informative plots for stakeholder communication
create_return_analysis_plots <- function(return_data) {
  # Price evolution plot
  price_plot <- ggplot(return_data, aes(x = date, y = price)) +
    geom_line(color = "steelblue", alpha = 0.8) +
    labs(
      title = "SPY Price Evolution (2015-Present)",
      subtitle = "Adjusted closing prices showing clear upward trend",
      x = "Date",
      y = "Price ($)",
      caption = "Source: Yahoo Finance via quantmod"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  # Return distribution plot
  return_hist <- ggplot(return_data, aes(x = return_pct)) +
    geom_histogram(bins = 50, fill = "lightblue", color = "darkblue", alpha = 0.7) +
    geom_vline(xintercept = mean(return_data$return_pct), 
               color = "red", linetype = "dashed", linewidth = 1) +
    labs(
      title = "Distribution of Daily Returns",
      subtitle = paste("Mean:", round(mean(return_data$return_pct), 3), "%, Std Dev:", 
                       round(sd(return_data$return_pct), 3), "%"),
      x = "Daily Return (%)",
      y = "Frequency",
      caption = "Red line shows mean return"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  # Time series of returns
  return_ts <- ggplot(return_data, aes(x = date, y = return_pct)) +
    geom_line(alpha = 0.6, color = "darkgreen") +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(
      title = "Daily Returns Time Series",
      subtitle = "Returns appear more stationary than prices",
      x = "Date",
      y = "Daily Return (%)"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  return(list(
    price_plot = price_plot,
    return_hist = return_hist,
    return_ts = return_ts
  ))
}

# Generate visualizations
plots <- create_return_analysis_plots(spy_returns)

# Display plots
plots$price_plot
plots$return_hist
plots$return_ts
```

```{r statistical-analysis}
# Domain Service: Statistical Analysis
# Responsibility: Provide comprehensive return statistics
analyze_return_properties <- function(return_data) {
  # Basic statistics
  basic_stats <- return_data %>%
    summarise(
      observations = n(),
      mean_return = mean(return_pct),
      std_dev = sd(return_pct),
      min_return = min(return_pct),
      max_return = max(return_pct),
      skewness = moments::skewness(return_pct),
      kurtosis = moments::kurtosis(return_pct),
      .groups = "drop"
    )
  
  # Annualized statistics (assuming 252 trading days per year)
  annualized_stats <- list(
    annual_mean = basic_stats$mean_return * 252,
    annual_volatility = basic_stats$std_dev * sqrt(252),
    sharpe_ratio = (basic_stats$mean_return * 252) / (basic_stats$std_dev * sqrt(252))
  )
  
  cat("=== RETURN ANALYSIS SUMMARY ===\n\n")
  cat("Daily Statistics:\n")
  cat("  Mean Return:", sprintf("%.4f%%", basic_stats$mean_return), "\n")
  cat("  Volatility: ", sprintf("%.4f%%", basic_stats$std_dev), "\n")
  cat("  Min Return: ", sprintf("%.2f%%", basic_stats$min_return), "\n")
  cat("  Max Return: ", sprintf("%.2f%%", basic_stats$max_return), "\n")
  cat("  Skewness:    ", sprintf("%.4f", basic_stats$skewness), "\n")
  cat("  Kurtosis:    ", sprintf("%.4f", basic_stats$kurtosis), "\n\n")
  
  cat("Annualized Statistics:\n")
  cat("  Annual Return:", sprintf("%.2f%%", annualized_stats$annual_mean), "\n")
  cat("  Annual Vol:    ", sprintf("%.2f%%", annualized_stats$annual_volatility), "\n")
  cat("  Sharpe Ratio: ", sprintf("%.4f", annualized_stats$sharpe_ratio), "\n\n")
  
  return(list(
    daily = basic_stats,
    annual = annualized_stats
  ))
}

# Perform return analysis
return_analysis <- analyze_return_properties(spy_returns)
```

# Exercise 3: State Vector Implementation

## State Vector Concept

According to our glossary, a **State** represents "all the information the agent sees right now when deciding." In our trading context, this is a window of recent returns that should satisfy the **Markov property** - containing sufficient information to predict what happens next.

```{r state-vector-factory}
# Domain Entity: State Vector Factory
# Responsibility: Encapsulate the current market state for agent decision-making
create_state_vector_factory <- function(window_size = 10) {
  # Validate window size
  if (!is.numeric(window_size) || window_size <= 0 || window_size != as.integer(window_size)) {
    stop("window_size must be a positive integer")
  }
  
  # Return factory function that creates state vectors
  function(return_data, t_index) {
    # Input validation
    if (!is.data.frame(return_data) || !"return_pct" %in% names(return_data)) {
      stop("return_data must be a data frame with 'return_pct' column")
    }
    
    if (!is.numeric(t_index) || length(t_index) != 1) {
      stop("t_index must be a single numeric value")
    }
    
    # Check if we have enough data to create the window
    if (t_index < window_size) {
      stop(paste("t_index must be at least", window_size, 
                 "to create a window of size", window_size))
    }
    
    if (t_index > nrow(return_data)) {
      stop(paste("t_index cannot exceed the number of observations:", nrow(return_data)))
    }
    
    # Calculate window boundaries
    # For t_index = 15 and window_size = 10, we want observations 6-15
    start_idx <- t_index - window_size + 1
    end_idx <- t_index
    
    # Extract the state vector
    state_vector <- return_data$return_pct[start_idx:end_idx]
    
    # Create metadata for the state
    state_metadata <- list(
      t_index = t_index,
      window_size = window_size,
      start_idx = start_idx,
      end_idx = end_idx,
      date_range = c(
        return_data$date[start_idx],
        return_data$date[end_idx]
      )
    )
    
    # Return state object with data and metadata
    structure(
      list(
        values = state_vector,
        metadata = state_metadata
      ),
      class = "MarketState"
    )
  }
}

# Create the state vector factory with window size 10
make_state <- create_state_vector_factory(window_size = 10)

# Test the implementation with the required example: t = 15 should look at bars 6-15
test_state <- make_state(spy_returns, t_index = 15)

cat("=== STATE VECTOR TEST (t_index = 15) ===\n\n")
cat("Expected: observations 6-15 (indices", 6, "to", 15, ")\n")
cat("Actual:    observations", test_state$metadata$start_idx, "to", test_state$metadata$end_idx, "\n")
cat("Window size:", test_state$metadata$window_size, "\n")
cat("Date range:", as.character(test_state$metadata$date_range[1]), "to", 
    as.character(test_state$metadata$date_range[2]), "\n\n")

cat("State vector values:\n")
print(round(test_state$values, 4))

# Verify our implementation matches the specification
verification_passed <- (test_state$metadata$start_idx == 6 && test_state$metadata$end_idx == 15)
cat("\nVerification:", ifelse(verification_passed, "PASSED ✓", "FAILED ✗"), "\n")
```

```{r state-validation}
# Domain Service: State Vector Validator
# Responsibility: Ensure state vectors meet quality requirements
validate_state_vector <- function(state_obj) {
  # Check if it's a proper MarketState object
  if (!inherits(state_obj, "MarketState")) {
    return(list(valid = FALSE, message = "Not a MarketState object"))
  }
  
  # Check if values are numeric and finite
  if (!is.numeric(state_obj$values) || any(!is.finite(state_obj$values))) {
    return(list(valid = FALSE, message = "State values must be finite numbers"))
  }
  
  # Check window size consistency
  expected_length <- state_obj$metadata$window_size
  actual_length <- length(state_obj$values)
  
  if (actual_length != expected_length) {
    return(list(valid = FALSE, 
                message = paste("Length mismatch: expected", expected_length, 
                                "got", actual_length)))
  }
  
  return(list(valid = TRUE, message = "State vector is valid"))
}

# Validate our test state
validation_result <- validate_state_vector(test_state)
cat("State validation:", validation_result$message, "\n")

# Demonstrate creating multiple states
cat("\n=== MULTIPLE STATE VECTOR EXAMPLES ===\n")

# Create several state vectors to show the sliding window concept
example_indices <- c(20, 50, 100, 200)

for (idx in example_indices) {
  if (idx <= nrow(spy_returns)) {
    state <- make_state(spy_returns, idx)
    cat("\nState at t =", idx, ":\n")
    cat("  Date:", as.character(state$metadata$date_range[2]), "\n")
    cat("  Window:", state$metadata$start_idx, "to", state$metadata$end_idx, "\n")
    cat("  Mean return in window:", round(mean(state$values), 4), "%\n")
    cat("  Std dev in window:", round(sd(state$values), 4), "%\n")
  }
}
```

```{r state-visualization}
# Domain Service: State Visualization
# Responsibility: Provide visual understanding of state vectors
visualize_state_vector <- function(return_data, t_index, window_size = 10) {
  # Create state for visualization
  state <- make_state(return_data, t_index)
  
  # Prepare data for plotting
  plot_data <- return_data %>%
    mutate(
      index = row_number(),
      in_window = index >= state$metadata$start_idx & index <= state$metadata$end_idx,
      is_current = index == t_index,
      # Create a new categorical variable for coloring and sizing points
      point_type = case_when(
        is_current ~ "Current",
        in_window ~ "In Window",
        TRUE ~ "Outside"
      )
    ) %>%
    filter(index >= (t_index - 20) & index <= (t_index + 5))  # Show context around the window
  
  # Create the plot
  ggplot(plot_data, aes(x = index, y = return_pct)) +
    geom_line(color = "lightgray", alpha = 0.5) +
    geom_point(aes(color = point_type, size = point_type)) +
    scale_color_manual(
      name = "State",
      values = c("In Window" = "steelblue", 
                 "Current" = "red", 
                 "Outside" = "lightgray")
    ) +
    scale_size_manual(
      name = "State",
      values = c("In Window" = 2, 
                 "Current" = 3, 
                 "Outside" = 1)
    ) +
    labs(
      title = paste("State Vector Visualization (t =", t_index, ")"),
      subtitle = paste("Window size:", window_size, "| Observations", 
                       state$metadata$start_idx, "to", state$metadata$end_idx),
      x = "Observation Index",
      y = "Return (%)",
      caption = "Red point = current time, Blue points = state window"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      legend.position = "bottom"
    )
}

# Visualize the test state
state_plot <- visualize_state_vector(spy_returns, t_index = 15)
state_plot
```

# Exercise 4: Neural Network Setup

Our Q-network will approximate the optimal Q-values for each action given a specific state. The input to the network will be the state vector (e.g., 10 recent returns), and the output will be the Q-values for each possible action (e.g., Buy, Sell, Hold).

```{r q-network-setup}
# Define actions available to the agent
# Domain Entity: Action
# Responsibility: Define possible actions for the agent
ACTIONS <- c("Buy", "Sell", "Hold")
NUM_ACTIONS <- length(ACTIONS)

# Domain Service: Q-Network Factory
# Responsibility: Create and manage the neural network architecture
create_q_network <- function(input_nodes, hidden_nodes, output_nodes) {
  # Input validation
  if (!all(is.numeric(c(input_nodes, hidden_nodes, output_nodes))) ||
      any(c(input_nodes, hidden_nodes, output_nodes) <= 0) ||
      any(c(input_nodes, hidden_nodes, output_nodes) != as.integer(c(input_nodes, hidden_nodes, output_nodes)))) {
    stop("All node counts must be positive integers.")
  }
  
  cat("\n=== Q-NETWORK SETUP ===\n")
  cat("Input Nodes (State Vector Size):", input_nodes, "\n")
  cat("Hidden Layers (Nodes per layer):", paste(hidden_nodes, collapse = ", "), "\n")
  cat("Output Nodes (Number of Actions):", output_nodes, "\n")
  
  # Prediction function for Q-values
  predict_q_values <- function(state_vector, nn_model = NULL) {
    if (is.null(nn_model)) {
      # If no model is provided (e.g., before training), return random Q-values
      # This simulates an untrained network's behavior
      q_values <- runif(output_nodes, min = -1, max = 1) # Random Q-values
      names(q_values) <- ACTIONS
      return(q_values)
    } else {
      # When a neuralnet model is provided, use its compute method
      # Ensure state_vector is a matrix, as expected by compute()
      if (!is.matrix(state_vector)) {
        state_vector <- matrix(state_vector, nrow = 1)
      }
      # Predict using the trained model
      q_values <- neuralnet::compute(nn_model, state_vector)$net.result
      colnames(q_values) <- ACTIONS # Assign names to output columns
      return(as.vector(q_values)) # Return as a named vector
    }
  }
  
  return(list(
    input_nodes = input_nodes,
    hidden_nodes = hidden_nodes,
    output_nodes = output_nodes,
    actions = ACTIONS,
    predict = predict_q_values # Return the prediction function
  ))
}

# Define Q-network parameters
WINDOW_SIZE <- 10 # From Exercise 3
q_network_params <- create_q_network(
  input_nodes = WINDOW_SIZE,
  hidden_nodes = c(64, 32), # Example: two hidden layers with 64 and 32 nodes
  output_nodes = NUM_ACTIONS
)

cat("\nQ-Network parameters defined. Ready for training.\n")
```

# Exercise 5: Artificial Q-Value Targets

In Q-learning, the agent learns by trying to match its predicted Q-values to a "target" Q-value. The target Q-value is typically calculated using the Bellman equation:

**Target Q(s,a) = Reward(s,a) + DiscountFactor × max(Q(s',a'))**

Where s' is the next state, and max(Q(s',a')) is the maximum Q-value for the next state.

```{r artificial-q-targets}
# Domain Service: Target Q-Value Generator
# Responsibility: Generate artificial target Q-values for a given state and action
generate_artificial_q_target <- function(state_vector, action_taken, reward, discount_factor = 0.95) {
  # Input validation
  if (!is.numeric(state_vector) || any(!is.finite(state_vector))) {
    stop("state_vector must be a numeric vector with finite values.")
  }
  if (!(action_taken %in% ACTIONS)) {
    stop("action_taken must be one of: ", paste(ACTIONS, collapse = ", "))
  }
  if (!is.numeric(reward) || length(reward) != 1) {
    stop("reward must be a single numeric value.")
  }
  if (!is.numeric(discount_factor) || length(discount_factor) != 1 || discount_factor < 0 || discount_factor > 1) {
    stop("discount_factor must be a single numeric value between 0 and 1.")
  }
  
  # For artificial targets, let's assume a simple reward structure:
  # If recent returns were positive, "Buy" might have a higher target.
  # If recent returns were negative, "Sell" might have a higher target.
  
  mean_return_in_state <- mean(state_vector)
  
  target_q_values <- numeric(NUM_ACTIONS)
  names(target_q_values) <- ACTIONS
  
  # Simple rule-based targets for demonstration:
  if (mean_return_in_state > 0.05) { # Arbitrary threshold for positive momentum
    target_q_values["Buy"] <- 1.0 # Strong positive
    target_q_values["Sell"] <- -0.5
    target_q_values["Hold"] <- 0.1
  } else if (mean_return_in_state < -0.05) { # Arbitrary threshold for negative momentum
    target_q_values["Buy"] <- -0.5
    target_q_values["Sell"] <- 1.0 # Strong positive
    target_q_values["Hold"] <- 0.1
  } else { # Sideways market
    target_q_values["Buy"] <- 0.2
    target_q_values["Sell"] <- 0.2
    target_q_values["Hold"] <- 0.5 # Hold is better in uncertain markets
  }
  
  # Add the immediate reward and adjust based on the action taken
  target_q_values[action_taken] <- target_q_values[action_taken] + reward
  
  return(target_q_values)
}

cat("\n=== ARTIFICIAL Q-VALUE TARGET GENERATION ===\n")
# Example usage:
# Get a test state (e.g., from t_index = 15)
current_state <- make_state(spy_returns, t_index = 15)$values
action <- "Buy" # Agent took 'Buy' action
immediate_reward <- 0.1 # Imagine a small positive reward

artificial_target_q <- generate_artificial_q_target(current_state, action, immediate_reward)
cat("Current State (first 5 returns):", paste(round(current_state[1:min(5, length(current_state))], 2), collapse = ", "), "...\n")
cat("Action Taken:", action, "\n")
cat("Immediate Reward:", immediate_reward, "\n")
cat("Generated Artificial Target Q-Values:\n")
print(round(artificial_target_q, 4))
```

# Exercise 6: Q-Learning Training Loop (Conceptual and Simplified)

The Q-learning training loop is an iterative process where the agent interacts with the environment, collects experience, and uses that experience to update its Q-network.

## Key Components of the Loop:

1. **Initialize Q-network**: Random weights at the start
2. **Experience Replay Buffer**: Store (state, action, reward, next_state, done) tuples
3. **Action Selection**: Epsilon-greedy policy for exploration/exploitation
4. **Interact with Environment**: Take action, observe reward and next state
5. **Sample Mini-batch**: From replay buffer
6. **Calculate Targets**: Using the Bellman equation
7. **Train Network**: Update Q-network weights to minimize the difference between predicted Q and target Q
8. **Update Target Network**: Periodically copy Q-network weights to a separate target network

```{r q-learning-training}
# Domain Service: Q-Learning Trainer (Simplified)
# Responsibility: Orchestrate the training process (conceptual)
train_q_network_simplified <- function(q_network_params, return_data,
                                       num_episodes = 5, steps_per_episode = 100,
                                       learning_rate = 0.01,
                                       epsilon_start = 1.0, epsilon_end = 0.01, epsilon_decay = 0.995) {
  
  cat("\n=== SIMPLIFIED Q-LEARNING TRAINING LOOP ===\n")
  cat("Number of Episodes:", num_episodes, "\n")
  cat("Steps per Episode:", steps_per_episode, "\n")
  cat("Initial Epsilon:", epsilon_start, "\n")
  
  current_epsilon <- epsilon_start
  
  # Dummy training data preparation for 'neuralnet'
  input_cols <- paste0("X", 1:q_network_params$input_nodes)
  output_cols <- q_network_params$actions
  
  # Create a formula string for neuralnet
  nn_formula_str <- paste(paste(output_cols, collapse = " + "), "~", paste(input_cols, collapse = " + "))
  nn_formula <- as.formula(nn_formula_str)
  
  # Collect some initial random data for training
  dummy_train_data <- as.data.frame(matrix(runif(100 * q_network_params$input_nodes, -1, 1), 
                                           ncol = q_network_params$input_nodes))
  colnames(dummy_train_data) <- input_cols
  dummy_train_data_output <- as.data.frame(matrix(runif(100 * q_network_params$output_nodes, -1, 1), 
                                                  ncol = q_network_params$output_nodes))
  colnames(dummy_train_data_output) <- output_cols
  dummy_train_data <- cbind(dummy_train_data, dummy_train_data_output)
  
  # Initial (untrained) Q-network
  initial_nn_model <- neuralnet(nn_formula, 
                                data = dummy_train_data, 
                                hidden = q_network_params$hidden_nodes, 
                                linear.output = TRUE, # Q-values are continuous
                                rep = 1 # Number of repetitions for different starting weights
  )
  
  q_network_model <- initial_nn_model
  
  for (episode in 1:num_episodes) {
    cat(paste0("\n--- Episode ", episode, " ---\n"))
    
    # Start at a random point in the data that allows for a full state vector
    start_idx <- sample(q_network_params$input_nodes:(nrow(return_data) - steps_per_episode - 1), 1)
    
    for (step in 1:steps_per_episode) {
      current_t_index <- start_idx + step - 1
      
      # 1. Get current state
      if (current_t_index + q_network_params$input_nodes > nrow(return_data)) {
        cat("  Not enough data for next state. Ending episode early.\n")
        break
      }
      current_state_obj <- make_state(spy_returns, t_index = current_t_index)
      current_state_vector <- current_state_obj$values
      
      # Normalize state vector (important for neural networks)
      min_ret <- min(spy_returns$return_pct, na.rm=TRUE)
      max_ret <- max(spy_returns$return_pct, na.rm=TRUE)
      normalized_state_vector <- (current_state_vector - min_ret) / (max_ret - min_ret)
      if (is.nan(normalized_state_vector) || (max_ret - min_ret) == 0) {
        normalized_state_vector <- rep(0.5, length(current_state_vector))
      }
      
      # 2. Select an action using epsilon-greedy strategy
      selected_action_idx <- select_action_epsilon_greedy(q_network_params$predict, normalized_state_vector,
                                                          current_epsilon, q_network_model)
      action_taken <- ACTIONS[selected_action_idx]
      
      # 3. Simulate Environment Interaction: Observe reward and next state
      next_return <- spy_returns$return_pct[current_t_index + 1]
      
      reward <- 0 # Base reward
      if (action_taken == "Buy") {
        reward <- next_return # If buy, reward is simply the next day's return
      } else if (action_taken == "Sell") {
        reward <- -next_return # If sell, reward is negative of next day's return
      } else if (action_taken == "Hold") {
        reward <- 0.01 * sign(next_return) # Small reward for holding if market moves favorably
      }
      
      # Cap reward to avoid extreme values
      reward <- min(max(reward, -1), 1)
      
      # 4. Get next state
      next_t_index <- current_t_index + 1
      
      # Check if we are at the end of the data
      is_done <- (next_t_index + q_network_params$input_nodes > nrow(return_data))
      
      next_state_vector <- NULL
      if (!is_done) {
        next_state_obj <- make_state(spy_returns, t_index = next_t_index)
        next_state_vector <- next_state_obj$values
        normalized_next_state_vector <- (next_state_vector - min_ret) / (max_ret - min_ret)
        if (is.nan(normalized_next_state_vector) || (max_ret - min_ret) == 0) {
          normalized_next_state_vector <- rep(0.5, length(next_state_vector))
        }
      } else {
        normalized_next_state_vector <- rep(0, q_network_params$input_nodes)
      }
      
      # 5. Calculate Target Q-value using the Bellman equation
      # Q_target = Reward + Gamma * max(Q(s', a'))
      
      if (!is_done) {
        q_values_next_state <- q_network_params$predict(normalized_next_state_vector, q_network_model)
        max_q_next_state <- max(q_values_next_state)
      } else {
        max_q_next_state <- 0 # No future reward if episode is done
      }
      
      # The target for the action taken in the current state
      target_q_for_action <- reward + 0.95 * max_q_next_state # Using discount_factor = 0.95
      
      # Get current Q-values for the current state
      current_q_predictions <- q_network_params$predict(normalized_state_vector, q_network_model)
      
      # Create target Q-vector for training
      target_q_vector <- current_q_predictions
      target_q_vector[selected_action_idx] <- target_q_for_action
      
      # 6. Train Network (conceptual update)
      # Create a data point for training
      train_input <- as.data.frame(t(normalized_state_vector))
      colnames(train_input) <- input_cols
      
      train_output <- as.data.frame(t(target_q_vector))
      colnames(train_output) <- output_cols
      
      training_data_point <- cbind(train_input, train_output)
      
      # Print update every 50 steps
      if (step %% 50 == 0) {
        cat(paste0("  Step ", step, ": Action=", action_taken, ", Reward=", round(reward, 3), 
                   ", Predicted Q[", action_taken, "]=", round(current_q_predictions[selected_action_idx], 3), 
                   ", Target Q=", round(target_q_for_action, 3), "\n"))
      }
    }
    
    # Epsilon decay
    current_epsilon <- max(epsilon_end, current_epsilon * epsilon_decay)
    cat(paste0("Episode ", episode, " finished. Epsilon decayed to: ", round(current_epsilon, 4), "\n"))
  }
  
  cat("\nSimplified Q-learning training loop complete.\n")
  return(q_network_model)
}
```

# Exercise 7: Action Selection Strategy (Epsilon-Greedy)

The epsilon-greedy strategy is a common method for balancing exploration and exploitation:

- With probability epsilon (ε), the agent explores by choosing a random action
- With probability (1 - ε), the agent exploits by choosing the action with the highest predicted Q-value
- Epsilon typically starts high (e.g., 1.0 for pure exploration) and decays over time to a small value (e.g., 0.01) to encourage more exploitation as the agent learns

```{r epsilon-greedy}
# Domain Service: Action Selector
# Responsibility: Choose an action based on epsilon-greedy strategy
select_action_epsilon_greedy <- function(predict_q_function, state_vector, epsilon, q_network_model) {
  # Input validation
  if (!is.function(predict_q_function)) {
    stop("predict_q_function must be a function to predict Q-values.")
  }
  if (!is.numeric(state_vector) || any(!is.finite(state_vector))) {
    stop("state_vector must be a numeric vector with finite values.")
  }
  if (!is.numeric(epsilon) || length(epsilon) != 1 || epsilon < 0 || epsilon > 1) {
    stop("epsilon must be a single numeric value between 0 and 1.")
  }
  
  if (runif(1) < epsilon) {
    # Explore: Choose a random action
    action_idx <- sample(1:NUM_ACTIONS, 1)
  } else {
    # Exploit: Choose the action with the highest Q-value
    q_values <- predict_q_function(state_vector, q_network_model)
    action_idx <- which.max(q_values)
  }
  return(action_idx)
}

cat("\n=== EPSILON-GREEDY ACTION SELECTION TEST ===\n")
# Get a test state (e.g., from t_index = 15)
test_state_for_action <- make_state(spy_returns, t_index = 15)$values

# Normalize for prediction
min_ret <- min(spy_returns$return_pct, na.rm=TRUE)
max_ret <- max(spy_returns$return_pct, na.rm=TRUE)
normalized_test_state <- (test_state_for_action - min_ret) / (max_ret - min_ret)
if (is.nan(normalized_test_state) || (max_ret - min_ret) == 0) {
  normalized_test_state <- rep(0.5, length(test_state_for_action))
}

# Test with high epsilon (exploration)
epsilon_high <- 0.9
cat(paste0("Testing with Epsilon = ", epsilon_high, " (High Exploration):\n"))
for (i in 1:5) {
  selected_action_idx <- select_action_epsilon_greedy(q_network_params$predict, normalized_test_state, epsilon_high, NULL)
  cat(paste0("  Trial ", i, ": Selected Action = ", ACTIONS[selected_action_idx], "\n"))
}

# Test with low epsilon (exploitation)
epsilon_low <- 0.1
cat(paste0("\nTesting with Epsilon = ", epsilon_low, " (High Exploitation):\n"))
for (i in 1:5) {
  selected_action_idx <- select_action_epsilon_greedy(q_network_params$predict, normalized_test_state, epsilon_low, NULL)
  cat(paste0("  Trial ", i, ": Selected Action = ", ACTIONS[selected_action_idx], "\n"))
}
cat("\nNote: Without a trained network, the 'exploit' choice will be based on random Q-values.\n")
cat("      This will become meaningful after the Q-network is trained.\n")
```

## Run Simplified Training Loop

This will execute the conceptual training. Note: The 'neuralnet' package is not designed for iterative online learning from a replay buffer. The training here is highly conceptual. For actual Deep Q-Learning, Keras/TensorFlow or PyTorch are necessary.

```{r run-training}
cat("\n\n--- INITIATING SIMPLIFIED TRAINING DEMONSTRATION ---\n")
final_q_network_model <- train_q_network_simplified(q_network_params, spy_returns,
                                                    num_episodes = 2, steps_per_episode = 100,
                                                    learning_rate = 0.01,
                                                    epsilon_start = 1.0, epsilon_end = 0.1, epsilon_decay = 0.9)

cat("\nSimplified training demonstration concluded.\n")
cat("You can now use the 'final_q_network_model' for predictions if needed, but remember its limitations in this conceptual example.\n")
```

