
---
title: "Neural Networks in Finance - Task 2"
author: "Group D"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org"))
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6
)

library(quantmod)
library(dplyr)
library(tibble)
library(ggplot2)
library(keras)
set.seed(42)
```

## Exercise 1: Create Extended State Vector

In this exercise, we extend the `make_state(t, pos)` function so that the output state vector includes both the **last 10 daily returns** and the **current position** of the agent.

###  Why include the position in the state?

In reinforcement learning, a **state** is a complete description of the situation the agent is currently in. For a trading agent, this means:

- The **market conditions**, represented by the last 10 returns.
- The **agentâ€™s internal state**, specifically whether it is already holding a position (long) or is flat.

If we do **not** include the position:
- The model cannot tell whether taking an action will open a position, close it, or maintain it.
- For example, staying "long" versus going "long" from a flat state will have different transaction costs or implications.

> By including the position, the agent learns to make decisions based on both market momentum **and** whether it is currently exposed to the market.

```{r}
make_state <- function(t, pos, returns, window_size = 10) {
  state_vector <- returns[(t - window_size + 1):t]
  c(state_vector, pos)
}
```

## Exercise 2: Understanding the Environment Theory

In reinforcement learning, the **environment** is everything that the agent interacts with. It provides the context in which the agent takes actions and receives feedback (called **rewards**).

In our trading scenario, the environment simulates a simplified financial market. It must contain all the necessary information that changes over time and affects how the agent behaves.

###  What does our environment keep track of?

Our environment keeps track of the following critical pieces of information:

1. **Time (`t`)** â€“ which day we're currently on in the dataset  
   - The state is always based on the last 10 returns, so we need to know where we are in time.
2. **Position (`pos`)** â€“ whether the agent is currently invested (1 = long) or not (0 = flat)  
   - This affects the reward and the outcome of actions.
3. **Equity (`equity`)** â€“ how much value the agentâ€™s portfolio has  
   - It changes over time depending on whether the agent makes or loses money.
4. **Done flag (`done`)** â€“ a boolean that tells us if weâ€™ve reached the end of the data  
   - Helps stop the simulation once there's no more return data to use.

###  Why is this important?

Without keeping track of this information, we cannot simulate:
- The effect of an action (buying or staying flat)
- The financial outcome of that action
- The transition to the next state (next returns + updated position)

All of this forms the feedback loop that lets the agent learn.

> In our case, the environment acts like a **financial simulator**. It gives the agent a new state after each action and tells the agent how good or bad the action was by returning a **reward**.

This setup aligns perfectly with the structure of reinforcement learning:
- The agent observes a **state**
- Takes an **action**
- Receives a **reward**
- Moves to a new **state**
- This cycle continues until the end (`done = TRUE`)

## Exercise 3: (Environment Application)

we begin coding our **trading environment** using two helper functions. These functions simulate how an agent interacts with the financial market from day to day.
### Part (a): `env_reset()`
This function simply initializes the environment:

We start at t = 10 because we need the last 10 returns to create the first state.

pos = 0 means we begin with no investment (flat).

equity = 1 means we start with normalized capital (like â‚¬1 or $1).

done = FALSE because the simulation is just beginning.

```{r}
env_reset <- function() {
  list(t = 10, pos = 0, equity = 1, done = FALSE)
}
```

## Part (b): env_step(env, action)
This function updates the environment after the agent takes an action. Here's how:

1. Advance time by one day (we go from t to t + 1).
2. Check the agent's position and action:
    If the agent buys (action == 1) and is not invested, they enter a position and
    pay transaction costs.
    If the agent is already invested and stays in the trade, they gain or lose based 
    on the return.
    If they sell (action == 0) while holding, they close the position and pay a 
    small cost.
3. Calculate new equity (total money) based on the reward.

4. Build the next environment state (new t, pos, equity, and done).

5. Create the next observation (state) using make_state().

6. Return everything as a list.


```{r}
getSymbols("SPY", src = "yahoo", from = "2015-01-01")
spy_returns <- dailyReturn(Cl(SPY)) * 100
returns <- as.numeric(spy_returns)
env_step <- function(env, action, transaction_cost = 0.0005) {
  t <- env$t
  pos <- env$pos
  equity <- env$equity

  if (t >= length(returns) - 1) {
    return(list(next_env = env, reward = 0, obs = rep(0, 11), done = TRUE))
  }

  r_t1 <- returns[t + 1] / 100
  reward <- 0

  if (action == 1 && pos == 0) {
    reward <- r_t1 - transaction_cost
    pos <- 1
  } else if (action == 0 && pos == 1) {
    reward <- -transaction_cost
    pos <- 0
  } else if (action == 1 && pos == 1) {
    reward <- r_t1
  }

  equity <- equity * (1 + reward)  # ðŸŸ¢ Apply reward to equity here

  t <- t + 1
  done <- (t >= length(returns) - 1)
  next_env <- list(t = t, pos = pos, equity = equity, done = done)
  obs <- make_state(t, pos, returns)
  list(next_env = next_env, reward = reward, obs = obs, done = done)
}

```
These two functions allow us to simulate a basic financial market.
The agent can take actions, earn or lose rewards, and progress through time.
This structure is essential for reinforcement learning to work. Without an environment, the agent wouldnâ€™t have anything to learn from!

## Exercise 4: Test Environment

In this exercise, we simulate a simple trading policy where the agent **enters a long position at time `t = 11`** and then **stays invested (long) for 10 periods**.

We want to observe how the agent's **equity** changes over time as a result of this strategy.

### ðŸ” Strategy
1. Starting from a clean environment (`env_reset()`).
2. At time `t = 11`, executing a **"buy"** action (`action = 1`) to enter the position.
3. For the next 9 time steps, **stay long** (`action = 1`) at each step.
4. Tracking the agentâ€™s **equity** at each step.

```{r}
# Resetting the environment to t = 10 (so we can act at t = 11)
env <- env_reset()

# Storing equity over time for plotting
equity_history <- numeric(11)
equity_history[1] <- env$equity

## Preparing Data
getSymbols("SPY", src = "yahoo", from = "2015-01-01", auto.assign = TRUE)
spy_prices <- Ad(SPY)
spy_df <- tibble(
  date = index(spy_prices),
  price = as.numeric(spy_prices)
)


# Step 1: Enter long at t = 11

returns <- spy_returns$return_percentage

step1 <- env_step(env, action = 1, transaction_cost = 0.0005)
env <- step1$next_env
equity_history[2] <- env$equity

# Steps 2â€“10: Stay long (action = 1)
for (i in 3:11) {
  step_result <- env_step(env, action = 1, transaction_cost = 0.0005)
  env <- step_result$next_env
  equity_history[i] <- env$equity
}

# Visualizing how equity evolved over time
equity_df <- data.frame(
  step = 1:11,
  equity = equity_history
)

ggplot(equity_df, aes(x = step, y = equity)) +
  geom_line(color = "blue", size = 1.2) +
  geom_point(color = "darkblue", size = 2) +
  labs(
    title = "Equity Over 10 Steps While Staying Long",
    x = "Step (Time t)",
    y = "Equity"
  ) +
  theme_minimal()
```
In the graph above, we see how the agent's money (equity) changes over 10 days when it buys and holds a stock (goes â€œlongâ€) and doesnâ€™t do anything else.

We start with equity = 1, meaning the agent begins with 1 unit of capital.

For the first few days, the equity increases. This means the stock price was going up, and the agent was making a profit just by holding the stock.

After that, the equity starts to fall. This shows the stock price went down for a few days, so the value of the agentâ€™s investment dropped.

At the end, we see small ups and downs â€” this is normal market movement. The agent didnâ€™t sell or take any action â€” it simply stayed invested the whole time.

Even with this simple strategy, we can see that the market affects the agentâ€™s returns. The agent earns more when the trend is positive and loses value when prices fall.

This experiment helps us understand how an agent's capital grows or shrinks over time when it follows a basic policy â€” in this case, buying and holding without reacting to changes.
## Exercise 5:Replay Buffer Initialization
To help the agent learn, we create a "replay buffer" that stores past experiences. This will be used later when we want to train the agent using batches of past data instead of only the most recent one.


```{r replay-buffer-init, message=FALSE}
state_size <- 11
dummy_state <- as.numeric(rep(0, state_size))

# (a) Creating a replay buffer using new.env()
replay_buffer <- new.env()

# Setting the buffer capacity
buffer_capacity <- 5000
state_size <- 11  # 10 returns + 1 position indicator

# (b) Initializing buffer elements
replay_buffer$s  <- matrix(0, nrow = buffer_capacity, ncol = state_size)   # states
replay_buffer$a  <- integer(buffer_capacity)                               # actions (0 or 1)
replay_buffer$r  <- numeric(buffer_capacity)                               # rewards
replay_buffer$s2 <- matrix(0, nrow = buffer_capacity, ncol = state_size)   # next states
replay_buffer$done <- logical(buffer_capacity)                             # whether episode ended

# Index to store next experience
replay_buffer$idx <- 1

# Boolean flag to check if buffer is full
replay_buffer$is_full <- FALSE

# Function to update index and check full condition
update_replay_index <- function(buffer) {
  buffer$idx <- buffer$idx + 1
  if (buffer$idx > buffer_capacity) {
    buffer$idx <- 1
    buffer$is_full <- TRUE
  }
}
# Simulate updating buffer index
update_replay_index(replay_buffer)

cat("Current index:", replay_buffer$idx, "\n")
cat("Is buffer full?:", replay_buffer$is_full, "\n")

```

## Exercise 6: Replay buffer functions

We enhance the functionality of our replay buffer to support training through two key operations. First, we implement the store_transition() function, which records the agentâ€™s current experienceâ€”namely the state (s), the action taken (a), the reward received (r), the next state (s0), and whether the episode has ended (done)â€”into the buffer at the current index. After storing, the index (idx) is updated to the next slot, and if the buffer has reached its maximum capacity (e.g., 5000 entries), the index loops back to the beginning, allowing the buffer to overwrite the oldest data in a circular fashion. This ensures that the buffer can continuously accommodate new experiences without growing indefinitely.

Second, we define the sample_batch(n) function, which randomly selects n past transitions from the replay buffer for training the agentâ€™s neural network. Random sampling breaks the correlation between consecutive experiences and improves training stability by ensuring a more diverse and representative learning signal. The function internally checks whether the buffer is full or partially filled and selects samples accordingly from the valid portion. These randomly selected batches simulate a more independent and identically distributed (i.i.d.) training process, which is crucial for reinforcement learning algorithms to perform well.

```{r}

store_transition <- function(buffer, s, a, r, s2, done) {
  idx <- buffer$idx
  
  buffer$s[idx, ]    <- s
  buffer$a[idx]      <- a
  buffer$r[idx]      <- r
  buffer$s2[idx, ]   <- s2
  buffer$done[idx]   <- done
  
  # Update index
  buffer$idx <- buffer$idx + 1
  if (buffer$idx > buffer_capacity) {
    buffer$idx <- 1
    buffer$is_full <- TRUE
  }
}
sample_batch <- function(buffer, n) {
  max_index <- if (buffer$is_full) buffer_capacity else (buffer$idx - 1)
  sampled_idx <- sample(1:max_index, size = n, replace = FALSE)
  
  list(
    s_batch    = buffer$s[sampled_idx, , drop = FALSE],
    a_batch    = buffer$a[sampled_idx],
    r_batch    = buffer$r[sampled_idx],
    s2_batch   = buffer$s2[sampled_idx, , drop = FALSE],
    done_batch = buffer$done[sampled_idx],
    idx_used   = sampled_idx  # Just for testing randomness
  )
}

set.seed(123)  # For reproducibility

# Simulate filling the buffer with dummy transitions

dummy_state <- rep(0, state_size)
for (i in 1:1000) {
  store_transition(replay_buffer, dummy_state, 0, 0, dummy_state, FALSE)
}

batch1 <- sample_batch(replay_buffer, n = 5)
batch2 <- sample_batch(replay_buffer, n = 5)

cat("Sample 1 indices:", batch1$idx_used, "\n")
cat("Sample 2 indices:", batch2$idx_used, "\n")
```
Since the indices are different between the two samples, it confirms that each call to sample_batch() gives uncorrelated (random) entries, which is good for training the neural network. It prevents the model from learning patterns that are just due to sequential data and helps generalize better.

## Exercise 7: Sanity Check

```{r}
env <- env_reset()

make_state <- function(t, pos, returns, window_size = 10) {
  past_returns <- returns[(t - window_size + 1):t]
  c(past_returns, pos)
}

env <- env_reset()
for (i in 1:10) {
  result <- env_step(env, action = 1)  # Agent stays long
  env <- result$next_env
}
cat("Number of transitions in Replay Buffer:", replay_buffer$idx - 1, "\n")
cat("Final Equity of Investor:", env$equity, "\n")
```
## Summary
In our trading environment, the done flag helps stop the simulation when we reach the end of the data. We start with equity = 1 to mean the investor begins with one unit of money, making it easy to track gains or losses. We begin at t = window_size (not t = 1) because the agent needs the last 10 returns to build a full state. If transaction costs doubled, we would reduce the reward more when trades happen, which makes the agent more careful about buying or selling. Finally, we sample random (uncorrelated) experiences from the replay buffer so the agent doesnâ€™t learn from only recent patternsâ€”this helps the model become more stable and smarter in the long run.
